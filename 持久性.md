# 持久性

## 第36 章 I/O 设备

### 系统架构

先看一个典型系统的架构：

![image-20220104191656489](https://s2.loli.net/2022/01/04/6VZGklTiugHsnxY.png)

CPU通过内存总线连接到系统内存，图像或者其它高性能I/O设备通过常规的I/O总线连接到系统，外围总线(SCSI,SATA,USB)将最慢的设备连接到系统中

采用这样的布局，是因为越短的总线越快，因此高性能的内存总线没有足够的空间连接太多设备，且高性能总线的造价很高，所以采用这种分层的布局，让要求高性能的设备(显卡)离CPU更近一点，低性能的设备离CPU远一点，将磁盘和其它低速设备连接到外围总线的好处有很多，如你可以在外围总线上连接大量的设备

### 标准设备

这是一个标准设备，通过它可以理解设备交互的机制，这个标准设备包含两部分重要组件

**硬件接口**：同软件一样，硬件也需要一些接口，让系统来控制它的操作，所有的设备都有自己的特定接口以及特有的交互协议

**内部结构**：包含设备功能的实现，一些非常简单的设备通常用一个或几个芯片来实现它们的功能，更复杂的设备会包含简单的CPU，一些通用内存，设备相关的特定芯片，来完成它们的工作，如现代RAID控制器通常包含上千行固件(硬件中的软件)
![image-20220104191807544](https://s2.loli.net/2022/01/04/Rs5MnIegQawJZuK.png)

### 标准协议

在标准设备的示意图中，设备接口包含了3个寄存器，一个状态寄存器(用于读取并查看当前设备的状态)，一个命令寄存器(用于通知设备执行某项任务)，一个数据寄存器(将数据传给设备或者从设备接收数据)，通过读写标准设备的这些寄存器，操作系统就可以控制该设备的行为

一个简单的交互协议：

```
//轮询设备当前状态
while(STATUS == BUSY) {
	;
}
//向数据寄存器和命令寄存器写入数据
Write data to DATA register
Write Command to COMMAND register
//轮询设备是否成功执行命令
while(STATUS == BUSY) {
	;
```


这个简单的标准协议包含4步：
1，操作系统反复读取状态寄存器，等待设备进入可以直接接收命令的就绪状态，称为轮询设备
2，操作系统下发数据到数据寄存器
3，操作系统将命令写入命令寄存器，这时设备就知道数据已经准备好了，它开始执行命令
4，操作系统不断轮询设备，等待并判断设备是否完成了命令(可能得到一个代表执行成功或失败的数据)

这个协议简单且有效，但难免有些低效和不方便，第一个问题就是轮询比较低效，在等待设备执行完成命令时浪费了大量CPU时间，如果此时操作系统切换到下一个就绪进程，就可以大大提高CPU的利用率

### 用中断减少CPU开销

利用中断可以减少CPU的开销，在上述的标准协议中，有了中断，CPU可以不用通过轮询设备来判断设备是否成功执行命令，而是向设备发出一个请求，然后让当前发起I/O的进程睡眠，切换执行其它进程，当设备执行完命令后，会抛出一个硬件中断，引发CPU跳转执行系统预先定义好的中断服务例程或中断处理程序，它会唤醒先前发起I/O的进程继续执行


因此中断允许计算与I/O重叠，这是提高CPU利用率的关键

但是使用中断也**并非是最佳**方案，考虑下面两个场景：
1，如果有一个非常高性能的设备，它处理请求很快，通常在CPU第一次轮询就能返回结果，如果此时使用中断，反而会让系统变慢，使用中断切换到其它进程，处理中断再切换回来带来了进程切换的开销，如果设备很快，那么最好的方法反而是轮询，如果设备较慢，那么采用允许发生重叠的中断更好，如果设备速度时慢时快，那么可以采用**混合策略，先轮询一小段时间，设备还没有完成命令时，再使用中断**
2，在网络中，网络端收到大量数据包，如果每个包引发一次中断，那么可能导致操作系统不断处理中断而无法处理用户的请求，**这种情况下，采用轮询可以更好控制系统的行为，让服务器先处理一些请求，再轮询网卡是否有数据包到达**

对于中断的处理也可以优化，通过合并，设备在抛出中断前先等待一小段时间，在此期间其它请求可能也会完成，就可以将多个中断合并成一次中断抛出，从而降低处理中断的代价

### 利用DMA进行更高效的数据传送

DMA引擎是操作系统中的一个特殊设备，它可以协调完成内存和设备间的数据传递，不需要CPU介入

DMA的工作过程：为了将数据传送给设备，操作系统通过编程告诉DMA引擎需要的数据所在内存的位置，要拷贝的大小以及要拷贝到哪个设备，之后操作系统就可以处理其他请求了，当DMA的任务完成后，DMA控制器会抛出一个中断告诉操作系统自己已经完成了数据传输

数据的拷贝都是由DMA完成的，因此CPU在此时是空闲的，所以操作系统可以让它做一些其它事情，如调度其它进程法

### 设备交互方法

**使用明确的I/O指令**：这些I/O指令规定了操作系统将数据发送到特定设备寄存器的方法，从而允许构造上文提到的协议

如在x86上，in和out指令都可以用来与设备进行交互，当需要发送数据给设备时，调用in指令指定一个存入数据的特定寄存器和一个代表设备的特定端口，执行这个指令就可以实现期望的行为

这些指令通常是特权指令，操作系统是唯一可以直接与设备交互的实体，不允许其它的程序直接读写磁盘，控制外设，这样会变得一团糟

**内存映射I/O**：通过这种方式，硬件将设备寄存器作为内存地址提供，当需要访问设备寄存器时，操作系统装载(读取)或者存入(写入)到该内存地址，然后硬件会将装载/存入转移到设备上，而不是物理内存

这两种方式没有一种具有极大的优势，内存映射I/O的好处是不需要引入新指令来实现设备交互，但两种方法都在使用

### 入操作系统：设备驱动程序

我们希望操作系统尽可能地通用，例如文件系统，我们希望开发一个文件系统可以工作在SCSI硬盘，IDE硬盘，USB设备等设备之上，并且希望这个文件系统不那么清楚对这些不同设备发出读写地全部细节

这个问题可以通过抽象来解决，在最底层，操作系统的一部分软件清楚地知道设备如何工作，将这部分软件称为**设备驱动程序**，所有设备交互的细节都封装在其中

Linux的文件系统栈：
文件系统完全不清楚它使用的是什么类型的磁盘，它只需要简单地向通用块设备层发送读写请求即可，设备层将这些请求交给设备驱动，然后设备驱动来完成真正的底层操作

这种封装也有不足的地方
1、如果一个设备可以提供很多特殊的功能，但因为兼容了大多数操作系统，它不得不提供一个通用的接口，**这样就使得自身的特殊功能无处使用**

2、因为所有需要插入系统的设备都需要安装对应的驱动程序，所以久而久之，**驱动程序在内核代码中占的比重越来越大**，Linux内核中70%都是各种驱动程序，因为驱动程序的开发者不是专业的内核开发人员，所以他们更容易写出缺陷，因此他们是**内核崩溃的主要贡献者**

### 简单的IDE磁盘驱动程序

看一个真实的设备-IDE磁盘驱动程序，IDE磁盘暴露给操作系统的接口比较简单，包含4种类型的寄存器，即控制，命令块，状态，错误，在x86上，利用I/O指令in和out向特定的I/O地址读取或写入时，可以访问这些寄存器

下面是与设备交互的简单协议，假设它已经初始化了：
1，**等待驱动就绪**，读取状态寄存器，直到驱动READY而非忙碌
2，**向命令寄存器写入参数**，写入扇区数，待访问扇区对应的逻辑块地址，并将驱动编号写入命令寄存器
3，**开启I/O**，发送读写命令到命令寄存器
4，**数据传送**(针对写请求)，等待驱动状态为READY
5，**中断处理**，完成后触发中断，恢复进程
6，**错误处理**，每次操作后读取状态寄存器，如果ERROR被置位，就可以读取错误寄存器获取详细信息

IDE读写主要通过4个函数实现：
1，ide_rw()，它会将一个请求加入队列，调用它的进程睡眠
2，ide_wait_ready()，确保驱动处于就绪状态
3，ide_start_request()，将请求发送到磁盘，进行in/out指令
4，ide intr()，完成后，发生中断，唤醒发起I/O的进程

## 第37章磁盘驱动器

磁盘驱动器（Hard Disk Driver）又称“磁盘机”，是以磁盘作为记录信息媒体的存储装置。它既是输入设备，又是输出设备

### 接口

现代磁盘驱动器的接口都很简单，磁盘驱动器由大量扇区(512字节)组成，每个扇区都可以读取或写入，在具有n个扇区的磁盘上，扇区从0到n-1编号，因此可以将磁盘视为一组扇区，从0到n-1是驱动器的地址空间

通常可以假设访问驱动器地址空间内两个彼此靠近的块比访问两个相隔很远的块更快，也可以假设访问连续块，是最快的访问模式，并且通常比任何随机的访问模式快得多

###  基本几何形状

![image-20220104192611954](https://s2.loli.net/2022/01/04/Giyt2adqlFPVsO7.png)

### 简单的磁盘驱动器

现在假设有一个单一磁道的简单磁盘：

该磁道只有12个扇区，每个扇区大小为512字节，用0-11表示这些扇区，这里的单个盘片围绕主轴旋转，电机连接到主轴，为了能读取或写入这些扇区，因此需要一个连接到磁盘臂上的磁头，磁盘表面逆时针旋转

![image-20220104192735115](https://s2.loli.net/2022/01/04/mgs2NWxERiGqZY9.png)

(**1) 单磁道延迟：旋转延迟**

现在加入收到了读取块0的请求，磁盘必须等待期望的扇区旋转到磁头下，这种等待在现代驱动器中经常发生，并且是I/O服务时间的重要组成部分，它有一个特殊的名称：旋转延迟

**(2) 多磁道：寻道时间**

![image-20220104192913854](https://s2.loli.net/2022/01/04/8MkQ3C2UDJoEwIn.png)单磁道是不现实的，现代磁盘有数以百万计的磁道，来看一个具有3条磁道的盘片表面：


现在追踪请求发生在远处扇区的情况，例如读取扇区11，此时磁头处于扇区30，在最内层的磁道，为了服务这个请求，驱动器首先将磁盘臂移动到正确的磁道，通过寻道过程，磁头被定位到了正确的磁道，接下来等待块11旋转经过磁头，当扇区11经过磁头时，I/O的最后阶段将发生，称为传输，数据从表面读取或写入表面

完整的I/O时间轨迹：寻道->等待转动延迟->传输

**(3) 一些其它细节**
1，外圈磁道通常比内圈磁道具有更多扇区，这是几何结构的结果，外圈磁道的空间更大
2，任何现代磁盘驱动器都有一个重要组成部分，即它的缓存，该缓存只是很少的内存，驱动器可以用这些内存来保存磁盘读取或写入磁盘的数据，

###  I/O时间

寻道：磁头从开始移动到数据所在**磁道**所需要的时间。寻道时间越短，I/O操作越快，

旋转延迟：盘片旋转将请求数据所在 扇区 移至读写 磁头 下方所需要的时间

数据传输时间：完成传输所请求的数据所需要的时间。

```
I/O时间T
T = T寻道 + T旋转 + T传输

I/O速率R
R = 传输大小 / T 

```

假设有两个工作负载，一个称为随机工作负载，它向磁盘上的随机位置发出小的读取请求，随机工作负载在许多重要的程序中很常见，包括数据库，第二种称为顺序工作负载，只是从磁盘连续读取大量的扇区，不会跳过，顺序访问也很常见

现在对希捷的一个高性能磁盘和一个大容量磁盘来估计这两个工作负载：

1，随机工作负载：
假设容量为4kb，在Cheetah上

```
T寻道 = 4ms //采用数据中的平均寻道时间
T旋转 = 2ms	//PRM=15000，则每次旋转需要4ms，半圈平均为2ms
T传输 = 30μs //传输大小除以最大传输速率
Cheetah的T = T寻道 + T旋转 + T传输 = 6ms
Cheetah的R = 4kb / T = 0.66MB/s

```

**在Barracuda上**

```
Barracuda的T = T寻道 + T旋转 + T传输 = 13.2ms
Barracuda的R = 4kb / T = 0.31MB/s
```

顺序工作负载：
假设容量为100MB，在Cheetah上

```
Cheetah的T = T寻道 + T旋转 + T传输 = 800ms
Cheetah的R = 100MB / T = 125MB/s
```

在Barracuda上

```
Barracuda的T = T寻道 + T旋转 + T传输 = 950ms
Barracuda的R = 100MB / T = 105MB/s
```


随机和顺序工作负载之间的驱动性能很大，对于Cheetah来说几乎是200倍左右，对于对于Barracuda来说差不多是300倍

###  磁盘调度

由于I/O的高成本，操作系统决定发送给I磁盘的I/O顺序方面发挥作用，即给定一组I/O请求，磁盘调度程序检查请求并决定下一个要调度的请求

与任务调度不同，每个任务的长度通常是不知道的，对于磁盘调度，可以很好的检测每个任务I/O请求的时间，通过估计请求的查找和可能的旋转延迟，磁盘调度程序可以知道每个请求会花费多长时间，因此将优先服务花费时间少的请求，因此磁盘调度程序对于一组I/O请求将尝试遵循SJF(最短任务优先)原则

**SSTF：最短寻道时间优先**
一种早期的调度方法被称为SSTF shortest seek time first，即最短寻道时间优先，SSTF按磁道对I/O请求队列排序，选择在最近磁道上的请求先完成

假设磁头在处于30扇区，现在的请求有21和2，根据SSTF，那么就会率先完成对21的请求，待扇区21的读写完成后，再处理对扇区2的请求

但是SSTF也存在一些缺陷：
1，操作系统并不知道驱动器的几何结构，在操作系统眼里整个驱动器只是一系列的块，采用SSTF并不能满足现代驱动器
2，饥饿，如果采用了SSTF，现在对邻近磁道上的块一直有请求，那么距离远的块的请求，就无法得到服务

 **SCAN：跨越磁道顺序**
为了解决饥饿，有一种算法称为SCAN，简单地以跨越磁道顺序来服务磁盘请求，将一次跨越磁盘称为扫一遍，如果请求的块所属的磁道在这次扫一遍中已经服务过了那么它就不会立刻处理，而是排队等待下次扫一遍

SCAN虽然能避免饥饿，但并没有严格遵循SJF原则，它忽视了旋转

**SPTF：最短定位时间优先**
SPTF shortest positioning time first，即最短定位时间优先，现代驱动器中，寻道和旋转大致相当，使用SPTF能提高性能

## 第38章—— 廉价冗余磁盘阵列 RAID

廉价冗余磁盘阵列(RAID)，这种技术使用多个磁盘一起构建更快，更大，更可靠的磁盘系统

RAID有许多好处，一个好处就是**性能**，并行使用多个磁盘可以大大加快I/O时间，另一个好处是**容量**，大型数据集需要大型磁盘，最后RAID可以提高**可靠性**，通过冗余，RAID允许损失一个磁盘并继续工作

### 接口和RAID内部

RAID看起来是一个快速的，可靠的，很大的磁盘。**它将自己展现为线性的块数组，每个块都可以通过文件系统读取或写入**

**当文件系统向RAID发出逻辑I/O请求时，RAID内部必须计算要访问的磁盘以完成请求，然后发出一个或多个物理I/O来执行此操作**

###  RAID的简单评估标准

容量，性能，可靠性

###  RAID 0级：条带化

第一个RAID级别实际上不是RAID级别，因为没有**冗余**，但是RAID 0级拥有高性能和大容量，所以值得了解

**RAID 0级的原理**

以轮转的方式将磁盘阵列的块分布在磁盘上，这种方法的目的是在对数组的连续块进行请求时，从阵列中获取最大的并行性(如在一个大的顺序读取中)，将同一行中的块称为条带，因此块0，1，2，3在相同的条带中

上述的示例中，假设同一行条带的每个磁盘只有一个块(4KB)，但是这不是必要的，也可以增大块的大小：

![image-20220104194624669](https://s2.loli.net/2022/01/04/P4qUnMiv3ZmDHfh.png)

大块大小不同的结果
**一方面，大块大小主要影响阵列的性能**，如大小较小的大块意为着许多文件将跨多个磁盘进行条带化，从而增加了对单个文件的读取和写入的并行性，但是跨多个磁盘访问块的定位时间会增加

**另一方面，较大的大块大小减少了这种文件内的并行性**，因此依靠多个并发请求来实现高吞吐量，但是较大的大块减少了定位时间

因此确定“最佳”大块大小是很难做到的，因为需要大量关于提供给磁盘系统的工作负载的知识
**RAID 0级的评估**

从容量的角度来看，RAID 0级是顶级的，给定N个磁盘，就有N个磁盘的可用空间.

从可靠性的角度来看，条带化最糟糕的是：任何磁盘故障都会导致数据丢失。

最后，其性能非常好，通常并行使用所有磁盘来为用户I/O请求提供服务

### RAID 1级：镜像

即**镜像**，对于镜像系统，只需要**生成系统中每个块的多个副本，每个副本放在一个单独的磁盘上，通过这样做，可以容许磁盘故障**

**RAID 1级的原理**

从镜像阵列读取块时，RAID有一个选择，它可以读取任一副本，如对RAID发出逻辑块5的读取，则可以自由地从磁盘2或磁盘3读取它，但是在**写入块时，RAID必须更新两个副本的数据，以保证可靠性，这种写入可以并行执行**

![image-20220104195048374](https://s2.loli.net/2022/01/04/7aGN2yKT6znlgbX.png)

**RAID 1级的评估**

从容量角度看，RAID 1级价格昂贵，在镜像级别等于2的情况下，只能获得峰值有用容量的一半，因此对于N个磁盘，镜像的有用容量为N/2。

从可靠性的角度看，RAID 1级表现良好，它可以容许任何一个磁盘的故障

最后，我们分析性能。从单个读取请求的延迟角度来看，我们可以看到它与单个磁盘上的延迟相同。写入有点不同：以它遭遇到两个请求中最差的寻道和旋转延迟，因此（平均而言）比写入单个磁盘略高。

### RAID 4级：通过奇偶校验节省空间

**RAID 4级的原理**
现在有一种向磁盘阵列添加冗余的不同方法，称为奇偶校验，基于奇偶校验的方法视图使用较少的容量，从而克服由镜像系统付出的巨大空间损失，不过这也会降低性能对于每一条数据，都添加一个奇偶校验块，用于存储该条块的冗余信息，如奇偶校验块P1具有块4，5，6，7计算出的冗余信息：
![image-20220104195611490](https://s2.loli.net/2022/01/04/CYT53n2zhWbHdVP.png)

**RAID 4级的评估**

从容量的角度来看，RAID 4级使用1个磁盘作为它所保护的每组磁盘的奇偶校验信息，因此RAID组的有效容量是N-1.

可靠性也很容易理解，RAID 4级允许1个磁盘故障，不允许更多，如果丢失多个磁盘，则无法重建丢失的数据

性能：

这些磁盘的数据位于磁盘0 和1 上，因此对数据的读写操作可以并行进行，这很好。出现的问题是奇偶校验磁盘。这两个请求都必须读取4 和13 的奇偶校验块，即奇偶校验块1 和3（用+标记）。估计你已明白了这个问题：在这种类型的工作负载下，奇偶校验磁盘是瓶颈。因此我们有时将它称为基于奇偶校验的RAID 的小写入问题（small-write problem）。
因此，即使可以并行访问数据磁盘，奇偶校验磁盘也不会实现任何并行。由于奇偶校验磁盘，所有对系统的写操作都将被序列化。由于奇偶校验磁盘必须为每个逻辑I/O 执行两次I/O（一次读取，一次写入），我们可以通过计算奇偶校验磁盘在这两个I/O 上的性能来计算RAID-4 中的小的随机写入的性能，从而得到（R / 2）MB/s。随机小写入下的RAID-4 吞吐量很糟糕，向系统添加磁盘也不会改善。

![image-20220104200703836](https://s2.loli.net/2022/01/04/6v4H7OkAgUdFlNB.png)

**延迟:**你现在知道，单次读取（假设没有失败）只映射到单个磁盘，因此其延迟等同于单个磁盘请求的延迟。单次写入的延迟需要两次读取，
然后两次写入。读操作可以并行进行，写操作也是如此，因此总延迟大约是单个磁盘的两
倍。（有一些差异，因为我们必须等待两个读取操作完成，所以会得到最差的定位时间，但
是之后，更新不会导致寻道成本，因此可能是比平均水平更好的定位成本。）

### RAID 5级：旋转奇偶校验

**RAID5与RAID4原理几乎相同，只是它将奇偶校验块跨驱动器旋转**

如你所见，每个条带的奇偶校验块现在都在磁盘上旋转，以消除RAID-4 的奇偶校验磁盘瓶颈。

![image-20220104200939827](https://s2.loli.net/2022/01/04/yRzGsjq7VPZ2UC6.png)

 **RAID 5级的评估**

RAID 5大部分与RAID 4相同，如两级的有效容量和容错能力是相同的，顺序读写性能也是如此，单个请求的延迟也与RAID 4相同

由于RAID 5基本上和RAID 4相同，只是少数情况下它更好，所以它几乎完全取代了市场上的RAID 4，唯一没有取代的地方是系统知道自己绝对不会执行大写入以外的任何事情，从而完全避免小写入问题

### RAID性能比较

我们将假设磁盘可以在连续工作负载下以S MB/s 传输数据，并且在随机工作负载下以R MB/s 传输数据。一般来说，S 比R 大得多。

![image-20220108210313664](https://s2.loli.net/2022/01/08/2acJ57dMneEFOrv.png)



![image-20220108210340613](https://s2.loli.net/2022/01/08/FE8QVhLjwtJaNZD.png)



## 第40章——文件系统的实现

关于文件系统的两个问题
理解文件系统时，需要考虑它们的两个不同方面：

**文件系统的数据结构**，即文件系统在磁盘上采用哪些类型的数据结构来组织其数据和元数据？较为简单的文件系统(如VSFS)采用简单的数据结构，如块和其它对象的数组，而复杂些的文件系统(如XFS)使用更复杂的基于树的结构

**访问方法**，即如何将进程发出的调用，如open()，read()，write()等映射到它的数据结构上？在执行特定系统调用期间读取哪些结构？改写哪些结构？这些步骤的执行效率如何？

### VSFS的整体组织

**1、将磁盘分成块**

简单的文件系统只使用一种块大小，这里也是这样，块大小选择4KB，对于构建简单文件系统的磁盘分区的做法很简单，**即将磁盘看成线性的一些块，每块大小为4KB**

在大小为N个4KB块的分区中，这些块的地址从0~N-1，假设有一个非常小的磁盘，只有64块

![image-20220104202058947](https://s2.loli.net/2022/01/04/AicRfgk2qUDF5aM.png)

**2、 用户数据的存放：数据区域**

有了一系列等大的线性块后，接着要像其中存储数据，首先是用户数据，**任何文件系统中大多数空间都应该是用户数据**，将用于存放用户数据的磁盘区域称为**数据区域**，这里将最后56个块作为数据区域：

![image-20220104202210608](https://s2.loli.net/2022/01/04/7ob1WJAqGM3BkTQ.png)

**3、记录每个文件的详细信息：inode表**

inode：记录了文件中包含哪些数据块，文件的大小，文件的所有者和访问权限，访问和修改时间等类似的详细信息，为了存储这些信息

为了存放inode，还需要在磁盘上留出一些空间，称这部分磁盘空间为inode表（inodemap），它只是保存了一个磁盘上inode的数组，假设inode表大小为5个块，磁盘看起来如下：

![image-20220104202429458](https://s2.loli.net/2022/01/04/q8uIwKtBgkRbC6E.png)

**4、空闲空间的记录：位图**

目前为止，文件系统有了数据块和inode表，但是还需要某种方法记录inode或数据块是空闲还是已分配，因此这种分配结构是所有文件系统中必需的部分

存在很多可行的分配方法，如采用空闲列表，指向第一个空闲块，然后它又指向下一个空闲块，这里采用一种简单又流行的结构
位图(位图是一种简单的结构，每个位用于指示相应的块是否空闲)，一种用于数据区域，一种用于inode表
![image-20220104202532655](https://s2.loli.net/2022/01/04/Aucng1mHXMvUsWe.png)

**文件系统信息的存储：超级块**

对于上述的磁盘布局，还有一块，这块留给**超级块**，**超级块包含关于该特定文件系统的信息**，包括例如文件系统中有多少个inode和数据块，inode表的开始位置，还可能包括一些幻数用来标识文件系统的类型(如VSFS)

![image-20220104202851081](https://s2.loli.net/2022/01/04/6bhgijvTuw4YcUN.png)

### 文件组织：inode(index node 索引节点)

**(1) inode结构寻址**
每个inode都由一个数字隐式引用，也称为文件的低级名称，如在VSFS中，给定一个文件低级名称，应该可以计算出该文件的inode结构在磁盘上的位置，根据之前的磁盘结构，假设inode表为20KB，因此最多存在80个inode结构，假设inode表从12KB开始，读取inode号32号：
文件系统首先开始计算inode区域的偏移量

![image-20220104203031326](https://s2.loli.net/2022/01/04/kJrB6mL9S3fcW8I.png)

(32*inode的大小1/4KB = 8192B)，
将它加上磁盘inode表的其实地址(12KB)，从而得到20KB
inode号为32的inode结构

**(3) inode结构中的内容**

在每个inode中，实际上是关于对应文件的所有信息：文件类型，大小，所得数据块数，权限，时间信息等

![image-20220104203210640](https://s2.loli.net/2022/01/04/LgKrTwuYmNjeVaC.png)

**(3) 多级索引**

设计inode时，最重要的决定之一是它如何引用数据块的位置，一种简单的方法是在inode中有一个或多个直接指针，每个指针指向属于该文件的一个磁盘，但当面对大文件时，直接指针不能满足需要：

当文件足够大时，分配一个间接块里面存储指向文件的块的指针，而inode结构中的间接指针则指向这个块，如果想支持更大的文件，可以使用双重间接指针，双重间接指针允许访问最大为102410244KB的文件，即所支持访问的文件大小超过了4GB，如果不够，还可以使用三重间接指针

许多文件系统使用多级索引，包括常用的文件系统，Linux的ext2和ext3，以及原始的UNIX等，多级索引的机制能让inode直接指向较小的文件，也可以通过一个或多个间接块指向大文件

### 目录组织

**一个目录基本上只包含一个二元组(条目名称，inode号)的列表**

加上dir中有3个文件，dir在磁盘上的数据可能如下显示：

![image-20220104204232785](https://s2.loli.net/2022/01/04/vEtcr8KV5zJPOe9.png)

**每个目录都有两个额外的条目，.(当前目录 dir) 和 …(父目录 /)**

通常文件系统将目录视为特殊类型的文件，目录有一个inode，位于inode表，该目录也有inode指向的数据块，这些数据块存在于我们简单文件系统的数据区域中

### 空闲空间管理

文件系统必须要记录哪些inode和数据块是空闲的，哪些不是，这样在分配文件或目录时，就可以为它找到空间，空闲空间管理对于所有文件系统都很重要，VSFS采用两个位图来完成

当创建一个文件时，必须为该文件分配一个inode，文件系统将通过位图来搜索一个空闲的inode结构并分配个该文件，文件系统将该inode结构标记为已经使用(位图中置1)
接着分配数据块时，大致是相同的，不过是根据数据块位图来寻找空闲空间，但是一些LInux文件系统在创建文件并需要数据块时，会寻找一系列数据块，通过查找一系列的数据块，将它们分配给创建的文件，文件系统保证该文件的一部分在磁盘上是连续的，从而提高性能，这种预分配策略，是为数据块分配空间时的常用方法

###  访问路径：读取和写入

**(1) 从磁盘读取**
假设要打开一个文件(如/foo/bar，该文件只有4KB)，读取后并关闭

第1步：**发出一个open("/foo/bar", O_RDONLY)调用时**，文件系统先要找到文件bar的inode 



第2步：**开始遍历，从根目录 / 开始**，文件系统第一次磁盘读取是根目录的inode=2

第3步：根**据inode中指向数据块的指针来查找条目foo**，一旦找到，文件系统也会找到下一个需要的foo的inode号

第4步：**递归遍历路径名，直到找到所需的inode**，本例中，文件系统读取包含foo的inode及其目录的数据的块，最后找到bar的inode



第5步：**open()将bar的inode读入内存**，然后文件系统对它进行最后的权限检查，在每个进程的打开文件表中，**为此进程分配一个文件描述符**，并将它返回给用户

第6步：**发出read()调用，从文件中读取**，第一次读取将在文件的第一个块中，读取将进一步更新此文件描述符在内存中的打开文件表，更新文件偏移量，以便下一次读取会读取第二个文件块

第7步：**关闭文件**，文件描述符被释放

整个过程中，打开文件导致了多次读取，以便找到目标文件的inode，之后，读取每个块需要文件系统先查询inode，然后读取该块，再使用写入更新inode的最后访问时间字段

**open()导致的I/O量与路径长度成正比**，对于路径中每个增加的目录，都需要读取它的inode及其数据，更糟的是会出现大型目录，对于大型目录需要读取更多的块

**(2) 向磁盘写入**
写入文件是一个类似的过程，首先文件必须被找到且打开，其次应用程序可以发出write()调用以更新文件内容，最后关闭文件

与读取不同，写入文件时可能会分配一个块，当写入一个新文件时，每次的操作不仅要将数据写入磁盘，还要先先决定将那个块分配给文件，从而更新磁盘的其它结构(数位图，inode表)，因此每次**写入文件在逻辑上有5个I/O：**

```
1，一个读取数据位图
2，一个写入位图
3，读取inode
4，写inode
5，写入数据块
```

创建一个文件的工作量比向已存在的文件中写数据更大，要创建一个文件，文件系统不仅要分别配一个inode，还要在包含新文件的目录中分配空间，这样的I/O总量非常大

###  缓存和缓冲

 **减少读的I/O：缓存**

读取和写入文件是昂贵的，会导致磁盘有很多I/O，因此降低性能，为了弥补这个问题，大多数文件系统积极使用系统内存 DRAM来缓存重要的块

想象一个有缓存的文件打开的例子，第一次可能引起很多I/O，来读取目录中的inode和数据，但是随后打开该文件，大部分会命中缓存，因此不需要或只进行少量I/O

**减少写的I/O：缓冲**
尽管可以通过足够大的缓存来避免读取/O，但写入操作必须进入磁盘，高速缓存不能减少写流量，可以通过写缓冲来弥补，首先，通过延迟写入，文件系统可以将零碎的一些更新积攒成一批，放入一组较小的I/O中，通过减少写I/O的次数来减少写I/O引起的问题

如果一在创建一个文件时，inode位图被更新，稍后再创建另一个文件时又被更新，则文件系统可以再第一次更新后延迟写入，从而节省一次I/O，其次，通过将一些写入缓冲的内存中，系统可以调度后续的I/O，从而提高性能

由于上述原因，大多数现代文件系统将写入在内存中缓冲5~30s，这代表了另一种折中，如果系统在更新传递到磁盘之前崩溃，更新就会丢失，但是如果时间延长，则可以通过批处理，调度甚至避免写入来提高性能，而像数据库管理系统这种软件不喜欢这种折中，它会调用fsync()立刻强制写入磁盘，以保证数据更新不会丢失

## 第42章——崩溃一致性：FSCK 和日志

文件系统面临的一个主要挑战在于，如何在出现断电（power loss）或系统崩溃（system crash）的情况下，更新持久数据结构。（为崩溃一致性问题（crash-consistency problem）。）

### **崩溃场景**

- 只将数据块（Db）写入磁盘
- 只有更新的inode（I[v2]）写入了磁盘
- 只有更新后的位图（B [v2]）写入了磁盘
- inode（I[v2]）和位图（B[v2]）写入了磁盘，但没有写入数据（Db）
- 写入了inode（I[v2]）和数据块（Db），但没有写入位图（B[v2]）
- 写入了位图（B[v2]）和数据块（Db），但没有写入inode（I[v2]）

在文件系统数据结构中可能存在不一致性。可能有**空间泄露**，可能**将垃圾数据返回给用户**，等等。理想的做法是将文件系统从一个一致状态（在文件被追加之前），原子地（atomically）移动到另一个状态（在inode、位图和新数据块被写入磁盘之后）。遗憾的是这不容易实现

### 解决方案1：文件系统检查程序

fsck 是一个UNIX 工具，用于查找这些不一致并修复它们。它在文件系
统挂载并可用之前运行（fsck 假定在运行时没有其他文件系统活动正在进行）。

以下是fsck 的基本总结：
**超级块**：fsck 首先检查超级块是否合理，主要是进行健全性检查，例如确保文件系统大小大于分配的块数。通常，这些健全性检查的目的是找到一个可疑的（冲突的）超级块。在这种情况下，系统（或管理员）可以决定使用超级块的备用副本。
 **空闲块**：接下来，fsck 扫描inode、间接块、双重间接块等，以了解当前在文件系统中分配的块。它利用这些知识生成正确版本的分配位图。因此，如果位图和inode之间存在任何不一致，则通过信任inode 内的信息来解决它。对所有inode 执行相同类型的检查，确保所有看起来像在用的inode，都在inode 位图中有标记。
 **inode 状态**：检查每个inode 是否存在损坏或其他问题。例如，fsck 确保每个分配的inode 具有有效的类型字段（即常规文件、目录、符号链接等）。如果inode 字段存在问题，不易修复，则inode 被认为是可疑的，并被fsck 清除，inode 位图相应地更新。
 **inode 链接**：fsck 还会验证每个已分配的inode 的链接数
 **重复**：fsck 还检查重复指针，即两个不同的inode 引用同一个块的情况。
**坏块**：在扫描所有指针列表时，还会检查坏块指针。如果指针显然指向超出其有效范围的某个指针，则该指针被认为是“坏的”，例如，它的地址指向大于分区大小的块。在这种情况下，fsck 不能做任何太聪明的事情。它只是从inode 或间接块中删除（清除）该指针。
**目录检查**：fsck 对每个目录的内容执行额外的完整性检查，确保“.”和“..”是前面的条目，目录条目中引用的每个inode 都已分配，并确保整个层次结构中没有目录的引用超过一次。

**缺点**：

1. 构建有效工作的fsck 需要复杂的文件系统知识。确保这样的代码在所有情
   况下都能正常工作可能具有挑战性
2. 它们太慢了。有效但低效

### 解决方案2：日志（或预写日志）

#### 数据日志

基本思路：

更新磁盘时，在覆写结构之前，首先写下一点小注记（在磁盘上的其他地方，在一个众所周知的位置），描述你将要做的事情。写下这个注记就是“预写”部分，我们把它写入一个结构，并组织成“日志”。因此，就有了**预写日志**。

日志功能在更新期间通过将注释写入磁盘增加了一工作量，从而大大减少了恢复期间所需的工作量。

**写入日志的文件系统协议**

1．**日志写入**：将事务的内容（包括TxB、元数据和数据）写入日志，等待这些写入完成。
2．**日志提交**：将事务提交块（包括TxE）写入日志，等待写完成，事务被认为已提交（committed）。
3．**加检查点**：将更新内容（元数据和数据）写入其最终的磁盘位置。

![一个例子](https://s2.loli.net/2022/01/04/3kWiFytMvcbPpzX.png)

#### **恢复**

1、**如果崩溃发生在事务被安全地写入日志之前**：简单地跳过待执行的更新。

2、**如果在事务已提交到日志之后但在加检查点完成之前**：
系统引导时，文件系统恢复过程将扫描日志，并查找已提交到磁盘的事务。然后，这些事务被重放（replayed，按顺序），文件系统再次尝试将事务中的块写入它们最终的磁盘位置。
这种形式为**重做日志**（redo logging）

#### **批处理日志更新**

一些文件系统不会一次一个地向磁盘提交每个更新。与此不同，可以将所有更新缓冲到全局事务中。

通过缓冲更新，文件系统在许多情况下可以避免对磁盘的过多的写入流量。

#### 使日志有限

一旦事务被加检查点，文件系统应释放它在日志中占用的空间，允许重用日志空间。

1．**日志写入**：将事务的内容（包括TxB 和更新内容）写入日志，等待这些写入完成。
2．**日志提交**：将事务提交块（包括TxE）写入日志，等待写完成，事务被认为已提交
（committed）。
3．**加检查点**：将更新内容写入其最终的磁盘位置。
4．**释放**：一段时间后，通过更新日志超级块，在日志中标记该事务为空闲。

#### 元数据日志

**最常用的是有序元数据日志**，它可以减少日志流量，同时仍然保证文件系统元数据和用户数据的合理一致性。

写入数据块的代价通常很昂贵，如何避免两次IO写入？

1．**数据写入**：将数据写入最终位置，等待完成（等待是可选的，详见下文）。
2．**日志元数据写入**：将开始块和元数据写入日志，等待写入完成。
3．**日志提交**：将事务提交块（包括TxE）写入日志，等待写完成，现在认为事务（包
括数据）已提交（committed）。
4．**加检查点元数据**：将元数据更新的内容写入文件系统中的最终位置。
5．**释放**：稍后，在日志超级块中将事务标记为空闲。
通过强制先写入数据，文件系统可以保证指针永远不会指向垃圾。

#### 棘手的情况：块复用

可以永远不再重复使用块，直到所述块的删除加上检查点，从日志中清除。Linux ext3 的做法是将新类型的记录添加到日志中，称为撤销（revoke）记录。在上面的情况中，删除目录将导致撤销记录被写入日志。在重放日志时，系统首先扫描这样的重新记录。任何此类被撤销的数据都不会被重放，从而避免了上述问题。

#### 其他方法

1. **软更新**
2. **写时复制（Copy-On-Write，COW）**
3. **基于反向指针的一致性**
4. **乐观崩溃一致性**



